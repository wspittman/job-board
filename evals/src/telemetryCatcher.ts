import { subscribeOpenAILogging } from "dry-utils-openai";
import { createHash } from "node:crypto";
import type { Context } from "./packagePortal";
import type { Scenario } from "./types";

/**
 * Catches and stores AI telemetry metrics.
 * It uses a hash of scenario details to mark and later find metrics for specific AI calls.
 */
class TelemetryCatcher {
  private logs: Record<string, Record<string, number>> = {};

  constructor() {}

  /**
   * Creates a modified input context for a scenario.
   * It adds a unique hash to the input item, which is used to identify the corresponding metrics later.
   * This hash is embedded in the data sent to the LLM.
   * @param scenario The scenario for which to create the marked input.
   * @returns A tuple containing the hash and the modified context.
   */
  createMarkedInput<T>({ runName, source }: Scenario<T>): [string, Context<T>] {
    const { sourceName, input } = source;
    const v = this.hash(runName, sourceName);

    return [
      v,
      {
        ...input,
        item: {
          // Add a simple hash to differentiate parallel requests.
          // This DOES affect what is sent to the LLM, but should be opaque to it.
          v,
          ...input.item,
        },
      },
    ];
  }

  /**
   * Stores a metrics entry.
   * The key for storing the metrics is extracted from the input string of the log.
   * This assumes the hash generated by `createMarkedInput` is present at a specific position in the input log.
   * @param tag The tag from the log, expected to contain the hash.
   * @param metrics The metrics to store.
   */
  catch(tag: string, metrics: Record<string, number>): void {
    // The key is the hash embedded in the input string (e.g., "item: { v: 'hash_value', ... }").
    // This extracts the 32-character hash.
    const key = tag.slice(6, 38);
    this.logs[key] = metrics;
  }

  /**
   * Finds metrics associated with a given context.
   * It uses the hash stored in the context's item to look up the metrics.
   * @param v The hash to look for.
   * @returns The found metrics, or undefined if none matches.
   */
  find(v: string): Record<string, number> | undefined {
    return this.logs[v];
  }

  /**
   * Generates a SHA256 hash from the given arguments and truncates it.
   * @param args Strings to include in the hash.
   * @returns A 32-character hexadecimal hash string.
   */
  private hash(...args: string[]): string {
    return createHash("sha256")
      .update(args.join(""))
      .digest("hex")
      .slice(0, 32);
  }
}

export const catcher = new TelemetryCatcher();

// Configure AI logging to use the TelemetryCatcher instance.
// This subscribes to logs from AI calls made via dry-utils-openai.
subscribeOpenAILogging({
  log: ({ tag, val }) => console.log(tag, val),
  error: ({ tag, val }) => console.error(new Error(tag, { cause: val })),
  aggregate: ({ dense, metrics }) =>
    catcher.catch(dense["in"] as string, metrics),
});
